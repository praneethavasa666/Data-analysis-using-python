{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMHyhp8MloHbMUSqyQGtYgG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"m41yVdI8jv6e"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"code","source":["#define image size and batch size\n","IMG_SIZE = 224\n","BATCH_SIZE = 32"],"metadata":{"id":"q8UrJ908i3_S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#creating training data parameters\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,\n","    validation_split = 0.2\n",")"],"metadata":{"id":"6CeRSHBai6SD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#creating traing data with above parameters\n","#folder = parameters,flow_from_directory(path,ts,bs,cm,subset)\n","train_generator = train_datagen.flow_from_directory(\n","    \"/content/drive/MyDrive/Brain_Tumor_Detection\",\n","     target_size = (IMG_SIZE,IMG_SIZE),\n","    batch_size = BATCH_SIZE,\n","    class_mode = 'binary',\n","    subset = 'training')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BM9j4Fmni-hd","executionInfo":{"status":"ok","timestamp":1709306316741,"user_tz":-330,"elapsed":8172,"user":{"displayName":"Rupa Tamarada","userId":"07489322130305110186"}},"outputId":"0c689358-ef73-42c1-c591-a5af96a9c3b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2448 images belonging to 2 classes.\n"]}]},{"cell_type":"code","source":["#creating validation data!\n","val_generator = train_datagen.flow_from_directory(\"/content/drive/MyDrive/Brain_Tumor_Detection/Train\",\n","      target_size = (IMG_SIZE,IMG_SIZE),\n","      batch_size = BATCH_SIZE,\n","      class_mode = 'binary',\n","      subset = 'validation')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DRDUaau6jCL6","executionInfo":{"status":"ok","timestamp":1709306321740,"user_tz":-330,"elapsed":427,"user":{"displayName":"Rupa Tamarada","userId":"07489322130305110186"}},"outputId":"482f6559-20c2-43b5-fdd2-f15fddfc7d18"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 600 images belonging to 2 classes.\n"]}]},{"cell_type":"code","source":["#Define the model\n","model = keras.Sequential([\n","    layers.Conv2D(32,(3,3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE,3)),\n","    layers.MaxPooling2D((2,2)),\n","    layers.Conv2D(64,(3,3), activation='relu'),\n","    layers.MaxPooling2D((2,2)),\n","    layers.Conv2D(128,(3,3), activation='relu'),\n","    layers.MaxPooling2D((2,2)),\n","    layers.Flatten(),\n","    layers.Dense(128,activation='relu'),\n","    layers.Dense(1,activation='sigmoid')\n","])"],"metadata":{"id":"S3jsev9YjGYV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n","\n"],"metadata":{"id":"MU01F2dbk1Uk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.fit(train_generator,validation_data=val_generator,epochs=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s9000TcyoBN_","executionInfo":{"status":"ok","timestamp":1709308096454,"user_tz":-330,"elapsed":1748908,"user":{"displayName":"Rupa Tamarada","userId":"07489322130305110186"}},"outputId":"d4f36742-b430-442c-f3d0-95127ad38e58"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1260: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n","  return dispatch_target(*args, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["77/77 [==============================] - 368s 5s/step - loss: 0.0000e+00 - accuracy: 0.0310 - val_loss: 0.0000e+00 - val_accuracy: 0.5000\n","Epoch 2/5\n","77/77 [==============================] - 325s 4s/step - loss: 0.0000e+00 - accuracy: 0.0196 - val_loss: 0.0000e+00 - val_accuracy: 0.5000\n","Epoch 3/5\n","77/77 [==============================] - 324s 4s/step - loss: 0.0000e+00 - accuracy: 0.0196 - val_loss: 0.0000e+00 - val_accuracy: 0.5000\n","Epoch 4/5\n","77/77 [==============================] - 321s 4s/step - loss: 0.0000e+00 - accuracy: 0.0196 - val_loss: 0.0000e+00 - val_accuracy: 0.5000\n","Epoch 5/5\n","77/77 [==============================] - 319s 4s/step - loss: 0.0000e+00 - accuracy: 0.0196 - val_loss: 0.0000e+00 - val_accuracy: 0.5000\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7a21739893f0>"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["model.save(\"Model.h5\",\"label.txt\")"],"metadata":{"id":"jFMgRgdInGGX"},"execution_count":null,"outputs":[]}]}